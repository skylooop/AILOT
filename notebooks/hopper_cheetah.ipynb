{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:45:31\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "os.environ['D4RL_SUPPRESS_IMPORT_ERROR'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "\n",
    "import gym\n",
    "import d4rl\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "from matplotlib import patches\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import functools\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from jaxrl_m.common import TrainStateEQX\n",
    "from src.agents.iql_equinox import GaussianPolicy, GaussianIntentPolicy\n",
    "\n",
    "from ott.geometry import pointcloud\n",
    "from ott.problems.linear import linear_problem\n",
    "from ott.solvers.linear import sinkhorn\n",
    "from ott.tools import plot, sinkhorn_divergence\n",
    "from ott.solvers.linear import implicit_differentiation as imp_diff\n",
    "\n",
    "import optax\n",
    "\n",
    "@eqx.filter_vmap(in_axes=dict(ensemble=eqx.if_array(0), s=None))\n",
    "def eval_ensemble_psi(ensemble, s):\n",
    "    return eqx.filter_vmap(ensemble.psi_net)(s)\n",
    "\n",
    "@eqx.filter_vmap(in_axes=dict(ensemble=eqx.if_array(0), s=None))\n",
    "def eval_ensemble_phi(ensemble, s):\n",
    "    return eqx.filter_vmap(ensemble.phi_net)(s)\n",
    "\n",
    "@eqx.filter_vmap(in_axes=dict(ensemble=eqx.if_array(0), s=None, g=None, z=None))\n",
    "def eval_ensemble_icvf_viz(ensemble, s, g, z):\n",
    "    return eqx.filter_vmap(ensemble.classic_icvf_initial)(s, g, z)\n",
    "\n",
    "@eqx.filter_vmap(in_axes=dict(ensemble=eqx.if_array(0), s=None, g=None, z=None)) # V(s, g, z), g - dim 29, z - dim 256\n",
    "def eval_ensemble_icvf_latent_z(ensemble, s, g, z):\n",
    "    return eqx.filter_vmap(ensemble.classic_icvf)(s, g, z)\n",
    "\n",
    "@eqx.filter_vmap(in_axes=dict(ensemble=eqx.if_array(0), s=None, g=None, z=None)) # V(s, g ,z ), g, z - dim 256\n",
    "def eval_ensemble_icvf_latent_zz(ensemble, s, g, z):\n",
    "    return eqx.filter_vmap(ensemble.icvf_zz)(s, g, z)\n",
    "    \n",
    "@eqx.filter_vmap(in_axes=dict(ensemble=eqx.if_array(0), s=None, g=None, z=None))\n",
    "def eval_ensemble_icvf_latent_zzz(ensemble, s, g, z):\n",
    "    return eqx.filter_vmap(ensemble.icvf_zzz)(s, g, z)\n",
    "\n",
    "@eqx.filter_vmap(in_axes=dict(ensemble=None, s=0, z=None))\n",
    "def eqx_get_state_traj(ensemble, s, z):\n",
    "    '''\n",
    "    Function to compute pairwise distance between two trajectories\n",
    "    '''\n",
    "    s = jnp.tile(s, (z.shape[0], 1))\n",
    "    return eval_ensemble_icvf_latent_zzz(icvf_model.value_learner.model, s, z, z)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def get_gcvalue(agent, s, g, z):\n",
    "    v_sgz_1, v_sgz_2 = eval_ensemble_icvf_viz(agent.value_learner.model, s, g, z)\n",
    "    return (v_sgz_1 + v_sgz_2) / 2\n",
    "\n",
    "def get_v_gz(agent, initial_state, target_goal, observations):\n",
    "    initial_state = jnp.tile(initial_state, (observations.shape[0], 1))\n",
    "    target_goal = jnp.tile(target_goal, (observations.shape[0], 1))\n",
    "    return -1 * get_gcvalue(agent, initial_state, observations, target_goal)\n",
    "    \n",
    "def get_v_zz(agent, goal, observations):\n",
    "    goal = jnp.tile(goal, (observations.shape[0], 1))\n",
    "    return get_gcvalue(agent, observations, goal, goal)\n",
    "\n",
    "@eqx.filter_vmap(in_axes=dict(agent=None, obs=None, goal=0))\n",
    "def get_v_zz_heatmap(agent, obs, goal): # goal - traj\n",
    "    goal = jnp.tile(goal, (obs.shape[0], 1))\n",
    "    return get_gcvalue(agent, obs, goal, goal)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 9/9 [00:02<00:00,  3.98it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5369d98d40c41caac421b277eb9df04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1998000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert returns [11239.283746674657], mean 11239.283746674657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 9/9 [00:02<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terminal states: 1\n",
      "Number of terminal states: 2001\n"
     ]
    }
   ],
   "source": [
    "from src.gc_dataset import GCSDataset\n",
    "from utils.ds_builder import setup_datasets\n",
    "\n",
    "env, expert_ds, agent_ds, agent_mean_states, agent_std_states = setup_datasets(expert_env_name=\"halfcheetah-medium-expert-v2\",\n",
    "                                          agent_env_name=\"halfcheetah-medium-expert-v2\", expert_num=1,\n",
    "                                          normalize_agent_states=False)\n",
    "\n",
    "gcsds_params = GCSDataset.get_default_config()\n",
    "gc_expert_dataset = GCSDataset(expert_ds, **gcsds_params)\n",
    "gc_agent_dataset = GCSDataset(agent_ds, **gcsds_params)\n",
    "\n",
    "expert_trajectory = gc_expert_dataset.get_expert_traj()['observations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nazar/projects/AILOT\n",
      "Extra kwargs: {}\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "from src.agents import icvf\n",
    "icvf_model = icvf.create_eqx_learner(seed=228,\n",
    "                                     observations=expert_ds.dataset_dict['observations'][0],\n",
    "                                     hidden_dims=[256, 256],\n",
    "                                     pretrained_folder=\"halfcheetah-medium-expert\",\n",
    "                                     load_pretrained_icvf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from jax.numpy import ndarray\n",
    "from ott.geometry import costs\n",
    "from ott.math import utils as mu\n",
    "\n",
    "\n",
    "class OTRewardsExpert:\n",
    "\n",
    "    def __init__(\n",
    "        self, expert_traj = None, expert_z = None\n",
    "    ):\n",
    "        self.expert_states = expert_traj\n",
    "        self.expert_z = expert_z\n",
    "        if expert_z is None:\n",
    "            self.expert_z = eval_ensemble_psi(icvf_model.value_learner.model, expert_traj).mean(axis=0)\n",
    "        self.sub_steps = 5\n",
    "\n",
    "    def make_subs(self, z, sub_steps):\n",
    "        sub_indx = jnp.minimum(jnp.arange(0, z.shape[0]) + sub_steps, z.shape[0] - 1)\n",
    "        return jax.tree_map(lambda arr: arr[sub_indx], z)\n",
    "    \n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def get_z_and_start_index(self, obs):\n",
    "        # obs - trajectory\n",
    "        z = eval_ensemble_psi(icvf_model.value_learner.model, obs).mean(axis=0)\n",
    "        diff = jnp.linalg.norm(z[0][jnp.newaxis,] - self.expert_z, axis=-1) #eqx_get_state_traj(icvf_model.value_learner.model, z[0][None], self.expert_z).mean(1)#z[0][jnp.newaxis,] - self.expert_z\n",
    "        i_min = jnp.argmin(diff)#jnp.argmin((diff**2).sum(-1)).squeeze()\n",
    "        return z, i_min, diff\n",
    "\n",
    "    def compute_rewards(\n",
    "        self,\n",
    "        dataset\n",
    "    ):\n",
    "        i0 = 0\n",
    "        rewards = []\n",
    "        observations = dataset.dataset_dict['observations']\n",
    "        episode_starts, episode_ends, episode_returns = gc_agent_dataset.dataset._trajectory_boundaries_and_returns()\n",
    "        \n",
    "        for i1 in tqdm(range(len(episode_starts))):\n",
    "            zi, start_index, diff = self.get_z_and_start_index(observations[episode_starts[i1]:episode_ends[i1]])\n",
    "            ri = self.compute_rewards_one_episode(zi, self.expert_z[start_index:])\n",
    "            #print(eval_ensemble_icvf_latent_zzz(icvf_model.value_learner.model, zi[0][None], self.expert_z[start_index][5][None], self.expert_z[start_index][5][None]).mean(0))\n",
    "            rewards.append(jax.device_get(ri))\n",
    "                  \n",
    "        return np.concatenate(rewards)#, selected_index\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def compute_rewards_one_episode(\n",
    "        self, episode_obs, expert_obs\n",
    "    ):\n",
    "\n",
    "        za_1 = episode_obs\n",
    "        za_2 = self.make_subs(za_1, self.sub_steps)\n",
    "        x = jnp.concatenate([za_1, za_2], axis=1)\n",
    "\n",
    "        ze_1 = expert_obs\n",
    "        ze_2 = self.make_subs(ze_1, self.sub_steps)\n",
    "        y = jnp.concatenate([ze_1, ze_2], axis=1)\n",
    "        \n",
    "        geom = pointcloud.PointCloud(x, y, epsilon=0.001)\n",
    "        ot_prob = linear_problem.LinearProblem(geom)\n",
    "        solver = sinkhorn.Sinkhorn(max_iterations=300, use_danskin=True)\n",
    "\n",
    "        ot_sink = solver(ot_prob)\n",
    "        transp_cost = jnp.sum(ot_sink.matrix * geom.cost_matrix, axis=1)\n",
    "        rewards = -transp_cost * 100\n",
    "\n",
    "        return rewards\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b8e6f659c54b34b3bf27e21464bde2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expert = OTRewardsExpert(expert_trajectory)\n",
    "rewards = expert.compute_rewards(gc_agent_dataset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 9/9 [00:02<00:00,  3.99it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b61a65c2e3647ab8bf28e5c4f7634d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1998000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.dataset import Dataset\n",
    "\n",
    "class ExpRewardsScaler:\n",
    "    def init(self, rewards: np.ndarray):\n",
    "        self.min = np.quantile(np.abs(rewards).reshape(-1), 0.0)\n",
    "        self.max = np.quantile(np.abs(rewards).reshape(-1), 0.95)\n",
    "\n",
    "    def scale(self, rewards: np.ndarray):\n",
    "        # From paper\n",
    "        return 2 * np.exp(3 * rewards)\n",
    "\n",
    "\n",
    "def get_subs(dataset: GCSDataset, add_steps: int):\n",
    "    terminal_locs = dataset.terminal_locs\n",
    "    indx = np.arange(dataset.dataset.dataset_dict['observations'].shape[0])\n",
    "    final_state_indx = terminal_locs[np.searchsorted(terminal_locs, indx)] \n",
    "    way_indx = np.minimum(indx + add_steps, final_state_indx)\n",
    "    subs = jax.tree_map(lambda arr: arr[way_indx], dataset.dataset.dataset_dict['observations'])\n",
    "    return subs\n",
    "\n",
    "sq_rewards = -jnp.sqrt(-rewards+0.0001)\n",
    "scaler = ExpRewardsScaler()\n",
    "#scaler.init(sq_rewards)\n",
    "scaled_rewards = scaler.scale(rewards).astype(np.float32)\n",
    "\n",
    "## Apply iql scaling\n",
    "from utils.ds_builder import load_trajectories\n",
    "    \n",
    "offline_traj = load_trajectories(env.spec.id, scaled_rewards)\n",
    "    \n",
    "def compute_iql_reward_scale(trajs):\n",
    "    \"\"\"Rescale rewards based on max/min from the dataset.\n",
    "    This is also used in the original IQL implementation.\n",
    "    \"\"\"\n",
    "    trajs = trajs.copy()\n",
    "    \n",
    "    def compute_returns(tr):\n",
    "        return sum([step[2] for step in tr])\n",
    "    \n",
    "    trajs.sort(key=compute_returns)\n",
    "    reward_scale = 1000.0 / (\n",
    "      compute_returns(trajs[-1]) - compute_returns(trajs[0]))\n",
    "    return reward_scale\n",
    "    \n",
    "\n",
    "ds = gc_agent_dataset.dataset.dataset_dict\n",
    "episode_starts, episode_ends, episode_returns = gc_agent_dataset.dataset._trajectory_boundaries_and_returns()\n",
    "data_with_ot_rewards = Dataset(\n",
    "    {'observations': np.concatenate([ds['observations'][episode_starts[i]:episode_ends[i]] for i, j in enumerate(episode_starts)]).astype(np.float32),#[scaled_rewards > r_min],\n",
    "    'next_observations': np.concatenate([ds['next_observations'][episode_starts[i]:episode_ends[i]] for i, j in enumerate(episode_starts)]).astype(np.float32),#[scaled_rewards > r_min],\n",
    "    'actions': np.concatenate([ds['actions'][episode_starts[i]:episode_ends[i]] for i, j in enumerate(episode_starts)]).astype(np.float32),#[scaled_rewards > r_min],\n",
    "    'rewards':scaled_rewards * compute_iql_reward_scale(offline_traj),\n",
    "    'masks': 1.0 - np.concatenate([ds['dones'][episode_starts[i]:episode_ends[i]] for i, j in enumerate(episode_starts)]).astype(np.float32),#[scaled_rewards > r_min],\n",
    "    })#[scaled_rewards > r_min]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56dd1c185e34df0a02259730b748fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'return': 4970.7735141067915, 'length': 1000.0}\n",
      "{'return': 5246.734164696242, 'length': 1000.0}\n",
      "{'return': 6973.7251702966005, 'length': 1000.0}\n",
      "{'return': 8053.332608825057, 'length': 1000.0}\n",
      "{'return': 5760.203850621122, 'length': 1000.0}\n",
      "{'return': 6794.748980632254, 'length': 1000.0}\n",
      "{'return': 6682.4577897749505, 'length': 1000.0}\n",
      "{'return': 5404.03986462656, 'length': 1000.0}\n",
      "{'return': 7782.59329851099, 'length': 1000.0}\n",
      "{'return': 7187.214399819459, 'length': 1000.0}\n",
      "{'return': 7103.503169974261, 'length': 1000.0}\n",
      "{'return': 6518.621234749199, 'length': 1000.0}\n",
      "{'return': 7400.920529688854, 'length': 1000.0}\n",
      "{'return': 7193.68911741444, 'length': 1000.0}\n",
      "{'return': 6669.739124283704, 'length': 1000.0}\n",
      "{'return': 7310.7417039440725, 'length': 1000.0}\n",
      "{'return': 8207.397912104232, 'length': 1000.0}\n",
      "{'return': 7619.996945590482, 'length': 1000.0}\n",
      "{'return': 7062.8304628408205, 'length': 1000.0}\n",
      "{'return': 9379.701922264492, 'length': 1000.0}\n"
     ]
    }
   ],
   "source": [
    "from src.agents.iql_flax.common import Batch\n",
    "from src.agents.iql_flax.learner import Learner\n",
    "from src.agents.iql_flax.evaluation import evaluate\n",
    "\n",
    "config={'max_steps': 1_000_000, 'seed': 10, 'expectile':0.7, 'discount': 0.99, 'temperature': 3}\n",
    "\n",
    "iql_agent_ot = Learner(\n",
    "        config[\"seed\"],\n",
    "        env.observation_space.sample()[np.newaxis],\n",
    "        env.action_space.sample()[np.newaxis],\n",
    "        max_steps=config[\"max_steps\"],\n",
    "        expectile=config[\"expectile\"],\n",
    "        discount=config[\"discount\"],\n",
    "        temperature=config[\"temperature\"])\n",
    "\n",
    "pbar = tqdm(range(1_000_000 + 1))\n",
    "# expert = OTRewardsExpert(expert_trajectory)\n",
    "\n",
    "for i in pbar:\n",
    "    sample = data_with_ot_rewards.sample(256)\n",
    "    batch = Batch(\n",
    "        observations=sample[\"observations\"],\n",
    "        next_observations=sample[\"next_observations\"],\n",
    "        actions = sample['actions'],\n",
    "        rewards= sample[\"rewards\"],\n",
    "        masks= sample[\"masks\"]\n",
    "    )\n",
    "    update_info = iql_agent_ot.update(batch)\n",
    "    update_info['adv'] = None\n",
    "    if i % 50_000 == 0 and i > 0:\n",
    "        eval_stats = evaluate(iql_agent_ot, env, num_episodes=10)[0]\n",
    "        print(eval_stats)\n",
    "        eval_stats['return'] = env.get_normalized_score(eval_stats['return'])*100\n",
    "        # wandb.log({f\"Eval/{key}\": value for key, value in eval_stats.items()})\n",
    "        pbar.set_postfix(update_info)\n",
    "    if i % 2000 == 0:\n",
    "        # wandb.log({f\"Training/{key}\": value for key, value in update_info.items()})\n",
    "        pbar.set_postfix(update_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'return': 92.64012068963244, 'length': 1000.0}\n"
     ]
    }
   ],
   "source": [
    "eval_stats = evaluate(iql_agent_ot, env, num_episodes=50)[0]\n",
    "eval_stats['return'] = env.get_normalized_score(eval_stats['return'])*100\n",
    "print(eval_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4841.356928073171\n",
      "Moviepy - Building video /home/nazar/projects/AILOT/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/nazar/projects/AILOT/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/nazar/projects/AILOT/rl-video-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.utils import save_video\n",
    "\n",
    "frames=[]\n",
    "i = 0\n",
    "num_episodes = 1\n",
    "all_reward = []\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    episode_reward = 0\n",
    "    key, sample_key = jax.random.split(key, 2)\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        key, sample_key = jax.random.split(sample_key, 2)\n",
    "        action = jax.device_get(iql_agent_ot.sample_actions(obs, temperature=0.0))\n",
    "        obs, reward, done ,_ = env.step(action)\n",
    "        os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "        os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "        episode_reward += reward\n",
    "    all_reward.append(episode_reward)\n",
    "    print(episode_reward)\n",
    "save_video.save_video(frames, video_folder='.', fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 9/9 [00:01<00:00,  6.36it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4675ddba80a4d27b72f2fb0b6806379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1998966 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert returns [3753.886583685875], mean 3753.886583685875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 9/9 [00:01<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terminal states: 1\n",
      "Number of terminal states: 3215\n"
     ]
    }
   ],
   "source": [
    "hopper_env, hopper_expert_ds, hopper_agent_ds, _, _ = setup_datasets(\n",
    "    expert_env_name=\"hopper-medium-expert-v2\",\n",
    "    agent_env_name=\"hopper-medium-expert-v2\", expert_num=1,\n",
    "    normalize_agent_states=False)\n",
    "\n",
    "hopper_gcsds_params = GCSDataset.get_default_config()\n",
    "hopper_gc_expert_dataset = GCSDataset(hopper_expert_ds, **hopper_gcsds_params)\n",
    "hopper_gc_agent_dataset = GCSDataset(hopper_agent_ds, **hopper_gcsds_params)\n",
    "\n",
    "hopper_expert_trajectory = hopper_gc_expert_dataset.get_expert_traj()['observations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nazar/projects/AILOT\n",
      "Extra kwargs: {}\n"
     ]
    }
   ],
   "source": [
    "%cd /home/nazar/projects/AILOT\n",
    "from src.agents import icvf\n",
    "hopper_icvf_model = icvf.create_eqx_learner(seed=228,\n",
    "                                     observations=hopper_expert_ds.dataset_dict['observations'][0],\n",
    "                                     hidden_dims=[256, 256],\n",
    "                                     pretrained_folder=\"hopper-medium-expert\",\n",
    "                                     load_pretrained_icvf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def crossenv_expert_projection(\n",
    "    expert_obs, agent_obs, other_agent_obs, icvf_model_1, icvf_model_2 \n",
    "):\n",
    "    \n",
    "    Ne = expert_obs.shape[0]\n",
    "    cat_obs = jnp.concatenate([expert_obs, agent_obs], axis=0)\n",
    "    z_1 = eval_ensemble_psi(icvf_model_1, cat_obs).mean(axis=0)\n",
    "    z_2 = eval_ensemble_psi(icvf_model_2, other_agent_obs).mean(axis=0)\n",
    "\n",
    "    geom = pointcloud.PointCloud(z_1, z_2, epsilon=0.001)\n",
    "    ot_prob = linear_problem.LinearProblem(geom)\n",
    "    solver = sinkhorn.Sinkhorn(max_iterations=500, use_danskin=True)\n",
    "\n",
    "    ot_sink = solver(ot_prob)\n",
    "    P = ot_sink.matrix[:Ne, :] \n",
    "    \n",
    "    expert_proj = P @ z_2\n",
    "\n",
    "    return expert_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheetah_obs = gc_agent_dataset.dataset.dataset_dict[\"observations\"][:30_000]\n",
    "hopper_obs = hopper_gc_agent_dataset.dataset.dataset_dict[\"observations\"][:30_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 11)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hopper_expert_trajectory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hopper_expert_projection = crossenv_expert_projection(\n",
    "    hopper_expert_trajectory, \n",
    "    hopper_obs, \n",
    "    cheetah_obs, \n",
    "    hopper_icvf_model.value_learner.model, \n",
    "    icvf_model.value_learner.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 256)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hopper_expert_projection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfc6ebe9dd140698758b96d503a418f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expert = OTRewardsExpert(expert_z=hopper_expert_projection)\n",
    "rewards = expert.compute_rewards(gc_agent_dataset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
