{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: May 20 2022 19:45:31\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "os.environ['D4RL_SUPPRESS_IMPORT_ERROR'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "\n",
    "import gym\n",
    "import d4rl\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "from matplotlib import patches\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import functools\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from jaxrl_m.common import TrainStateEQX\n",
    "from src.agents.iql_equinox import GaussianPolicy, GaussianIntentPolicy\n",
    "\n",
    "from ott.geometry import pointcloud\n",
    "from ott.problems.linear import linear_problem\n",
    "from ott.solvers.linear import sinkhorn\n",
    "from ott.tools import plot, sinkhorn_divergence\n",
    "from ott.solvers.linear import implicit_differentiation as imp_diff\n",
    "\n",
    "import optax\n",
    "\n",
    "@eqx.filter_vmap(in_axes=dict(ensemble=eqx.if_array(0), s=None))\n",
    "def eval_ensemble_psi(ensemble, s):\n",
    "    return eqx.filter_vmap(ensemble.psi_net)(s)\n",
    "\n",
    "@eqx.filter_vmap(in_axes=dict(ensemble=eqx.if_array(0), s=None))\n",
    "def eval_ensemble_phi(ensemble, s):\n",
    "    return eqx.filter_vmap(ensemble.phi_net)(s)\n",
    "\n",
    "@eqx.filter_vmap(in_axes=dict(ensemble=eqx.if_array(0), s=None, g=None, z=None))\n",
    "def eval_ensemble_icvf_viz(ensemble, s, g, z):\n",
    "    return eqx.filter_vmap(ensemble.classic_icvf_initial)(s, g, z)\n",
    "\n",
    "@eqx.filter_vmap(in_axes=dict(ensemble=eqx.if_array(0), s=None, g=None, z=None)) # V(s, g, z), g - dim 29, z - dim 256\n",
    "def eval_ensemble_icvf_latent_z(ensemble, s, g, z):\n",
    "    return eqx.filter_vmap(ensemble.classic_icvf)(s, g, z)\n",
    "\n",
    "@eqx.filter_vmap(in_axes=dict(ensemble=eqx.if_array(0), s=None, g=None, z=None)) # V(s, g ,z ), g, z - dim 256\n",
    "def eval_ensemble_icvf_latent_zz(ensemble, s, g, z):\n",
    "    return eqx.filter_vmap(ensemble.icvf_zz)(s, g, z)\n",
    "    \n",
    "@eqx.filter_vmap(in_axes=dict(ensemble=eqx.if_array(0), s=None, g=None, z=None))\n",
    "def eval_ensemble_icvf_latent_zzz(ensemble, s, g, z):\n",
    "    return eqx.filter_vmap(ensemble.icvf_zzz)(s, g, z)\n",
    "\n",
    "@eqx.filter_vmap(in_axes=dict(ensemble=None, s=0, z=None))\n",
    "def eqx_get_state_traj(ensemble, s, z):\n",
    "    '''\n",
    "    Function to compute pairwise distance between two trajectories\n",
    "    '''\n",
    "    s = jnp.tile(s, (z.shape[0], 1))\n",
    "    return eval_ensemble_icvf_latent_zzz(icvf_model.value_learner.model, s, z, z)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def get_gcvalue(agent, s, g, z):\n",
    "    v_sgz_1, v_sgz_2 = eval_ensemble_icvf_viz(agent.value_learner.model, s, g, z)\n",
    "    return (v_sgz_1 + v_sgz_2) / 2\n",
    "\n",
    "def get_v_gz(agent, initial_state, target_goal, observations):\n",
    "    initial_state = jnp.tile(initial_state, (observations.shape[0], 1))\n",
    "    target_goal = jnp.tile(target_goal, (observations.shape[0], 1))\n",
    "    return -1 * get_gcvalue(agent, initial_state, observations, target_goal)\n",
    "    \n",
    "def get_v_zz(agent, goal, observations):\n",
    "    goal = jnp.tile(goal, (observations.shape[0], 1))\n",
    "    return get_gcvalue(agent, observations, goal, goal)\n",
    "\n",
    "@eqx.filter_vmap(in_axes=dict(agent=None, obs=None, goal=0))\n",
    "def get_v_zz_heatmap(agent, obs, goal): # goal - traj\n",
    "    goal = jnp.tile(goal, (obs.shape[0], 1))\n",
    "    return get_gcvalue(agent, obs, goal, goal)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 21/21 [00:01<00:00, 17.96it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9590b22aac7147ad9e2ad961e14f6cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/999000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert returns [11239.283746674657], mean 11239.283746674657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 21/21 [00:01<00:00, 18.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terminal states: 1\n",
      "Number of terminal states: 1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 21/21 [00:00<00:00, 29.22it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1b02162814454e80374b29fabd9ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/999061 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert returns [3753.886583685875], mean 3753.886583685875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 21/21 [00:00<00:00, 29.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terminal states: 1\n",
      "Number of terminal states: 2188\n"
     ]
    }
   ],
   "source": [
    "from src.gc_dataset import GCSDataset\n",
    "from utils.ds_builder import setup_datasets\n",
    "\n",
    "class EnvData:\n",
    "    def __init__(self, env, agent_dataset, expert_dataset, expert_trajectory) -> None:\n",
    "        self.env = env\n",
    "        self.agent_dataset = agent_dataset\n",
    "        self.expert_dataset = expert_dataset\n",
    "        self.expert_trajectory = expert_trajectory\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def expert_intents(self, icvf_value_model):\n",
    "        return eval_ensemble_psi(icvf_value_model, self.expert_trajectory).mean(axis=0)\n",
    "\n",
    "def load_env_data(expert_env_name, agent_env_name, expert_num):\n",
    "\n",
    "    env, expert_ds, agent_ds, agent_mean_states, agent_std_states = setup_datasets(\n",
    "        expert_env_name=expert_env_name,\n",
    "        agent_env_name=agent_env_name, \n",
    "        expert_num=expert_num,\n",
    "        normalize_agent_states=False)\n",
    "\n",
    "    gcsds_params = GCSDataset.get_default_config()\n",
    "    gc_expert_dataset = GCSDataset(expert_ds, **gcsds_params)\n",
    "    gc_agent_dataset = GCSDataset(agent_ds, **gcsds_params)\n",
    "\n",
    "    expert_trajectory = gc_expert_dataset.get_expert_traj()['observations']\n",
    "\n",
    "    return EnvData(env, gc_agent_dataset.dataset, gc_expert_dataset.dataset, expert_trajectory)\n",
    "\n",
    "halfcheetah_data = load_env_data(\"halfcheetah-expert-v2\", \"halfcheetah-medium-v2\", expert_num=1)\n",
    "hopper_data = load_env_data(\"hopper-expert-v2\", \"hopper-medium-v2\", expert_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nazar/projects/AILOT\n",
      "Extra kwargs: {}\n",
      "Extra kwargs: {}\n"
     ]
    }
   ],
   "source": [
    "%cd /home/nazar/projects/AILOT\n",
    "from src.agents import icvf\n",
    "halfcheetah_icvf_model = icvf.create_eqx_learner(seed=228,\n",
    "                                     observations=halfcheetah_data.expert_dataset.dataset_dict['observations'][0],\n",
    "                                     hidden_dims=[256, 256],\n",
    "                                     pretrained_folder=\"halfcheetah-medium\",\n",
    "                                     load_pretrained_icvf=True)\n",
    "\n",
    "hopper_icvf_model = icvf.create_eqx_learner(seed=228,\n",
    "                                     observations=hopper_data.expert_dataset.dataset_dict['observations'][0],\n",
    "                                     hidden_dims=[256, 256],\n",
    "                                     pretrained_folder=\"hopper-medium\",\n",
    "                                     load_pretrained_icvf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from jax.numpy import ndarray\n",
    "from ott.geometry import costs\n",
    "from ott.math import utils as mu\n",
    "\n",
    "\n",
    "class OTRewardsExpert:\n",
    "\n",
    "    def __init__(\n",
    "        self, icvf_model, expert_z\n",
    "    ):\n",
    "        self.icvf_model = icvf_model\n",
    "        self.expert_z = expert_z\n",
    "        self.sub_steps = 1\n",
    "\n",
    "    def make_subs(self, z, sub_steps):\n",
    "        sub_indx = jnp.minimum(jnp.arange(0, z.shape[0]) + sub_steps, z.shape[0] - 1)\n",
    "        return jax.tree_map(lambda arr: arr[sub_indx], z)\n",
    "    \n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def get_z_and_start_index(self, obs):\n",
    "        # obs - trajectory\n",
    "        z = eval_ensemble_psi(self.icvf_model.value_learner.model, obs).mean(axis=0)\n",
    "        diff = jnp.linalg.norm(z[0][jnp.newaxis,] - self.expert_z, axis=-1) \n",
    "        i_min = jnp.argmin(diff)\n",
    "        return z, i_min, diff\n",
    "\n",
    "    def compute_rewards(\n",
    "        self,\n",
    "        dataset\n",
    "    ):\n",
    "        i0 = 0\n",
    "        rewards = []\n",
    "        observations = dataset.dataset_dict['observations']\n",
    "        episode_starts, episode_ends, _ = dataset._trajectory_boundaries_and_returns()\n",
    "        \n",
    "        for i1 in tqdm(range(len(episode_starts))):\n",
    "            zi, start_index, diff = self.get_z_and_start_index(observations[episode_starts[i1]:episode_ends[i1]])\n",
    "            ri = self.compute_rewards_one_episode(zi, self.expert_z[start_index:])\n",
    "            rewards.append(jax.device_get(ri))\n",
    "                  \n",
    "        return np.concatenate(rewards)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def compute_rewards_one_episode(\n",
    "        self, episode_obs, expert_obs\n",
    "    ):\n",
    "\n",
    "        za_1 = episode_obs\n",
    "        za_2 = self.make_subs(za_1, self.sub_steps)\n",
    "        x = jnp.concatenate([za_1, za_2], axis=1)\n",
    "\n",
    "        ze_1 = expert_obs\n",
    "        ze_2 = self.make_subs(ze_1, self.sub_steps)\n",
    "        y = jnp.concatenate([ze_1, ze_2], axis=1)\n",
    "        \n",
    "        geom = pointcloud.PointCloud(x, y, epsilon=0.001)\n",
    "        ot_prob = linear_problem.LinearProblem(geom)\n",
    "        solver = sinkhorn.Sinkhorn(max_iterations=500, use_danskin=True)\n",
    "\n",
    "        ot_sink = solver(ot_prob)\n",
    "        transp_cost = jnp.sum(ot_sink.matrix * geom.cost_matrix, axis=1)\n",
    "        rewards = -transp_cost * 100\n",
    "\n",
    "        return rewards\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import Dataset\n",
    "\n",
    "class ExpRewardsScaler:\n",
    "\n",
    "    def __init__(self, a=2.0, b=1.0):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def scale(self, rewards: np.ndarray):\n",
    "        # From paper\n",
    "        return self.a * np.exp(rewards * self.b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_iql_reward_scale(dataset, rewards):\n",
    "\n",
    "    episode_starts, episode_ends, _ = dataset._trajectory_boundaries_and_returns()\n",
    "    episode_lens = [i2 - i1 for i1, i2 in zip(episode_starts, episode_ends)]\n",
    "    episode_sep = np.cumsum([0] + episode_lens)\n",
    "    \n",
    "    r_sum = [rewards[i1:i2].sum() for i1, i2 in zip(episode_sep[:-1],  episode_sep[1:])]\n",
    "    reward_scale = 1000.0 / (max(r_sum) - min(r_sum))\n",
    "    return reward_scale\n",
    "\n",
    "def make_train_data(env_data: EnvData, rewards, scaler: ExpRewardsScaler):\n",
    "\n",
    "    scaled_rewards = scaler.scale(rewards).astype(np.float32)\n",
    "    print(rewards.min(), rewards.mean(), rewards.max())\n",
    "    print(scaled_rewards.min(), scaled_rewards.mean(), scaled_rewards.max())\n",
    "\n",
    "    ds = env_data.agent_dataset.dataset_dict\n",
    "    episode_starts, episode_ends, _ = env_data.agent_dataset._trajectory_boundaries_and_returns()\n",
    "\n",
    "    data_with_ot_rewards = Dataset(\n",
    "        {'observations': np.concatenate([ds['observations'][episode_starts[i]:episode_ends[i]] for i, j in enumerate(episode_starts)]).astype(np.float32),#[scaled_rewards > r_min],\n",
    "        'next_observations': np.concatenate([ds['next_observations'][episode_starts[i]:episode_ends[i]] for i, j in enumerate(episode_starts)]).astype(np.float32),#[scaled_rewards > r_min],\n",
    "        'actions': np.concatenate([ds['actions'][episode_starts[i]:episode_ends[i]] for i, j in enumerate(episode_starts)]).astype(np.float32),#[scaled_rewards > r_min],\n",
    "        'rewards':scaled_rewards * compute_iql_reward_scale(env_data.agent_dataset, scaled_rewards),\n",
    "        'masks': 1.0 - np.concatenate([ds['dones'][episode_starts[i]:episode_ends[i]] for i, j in enumerate(episode_starts)]).astype(np.float32),#[scaled_rewards > r_min],\n",
    "        })\n",
    "    return data_with_ot_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.iql_flax.common import Batch\n",
    "from src.agents.iql_flax.learner import Learner\n",
    "from src.agents.iql_flax.evaluation import evaluate\n",
    "\n",
    "def training(env_data: EnvData, data_with_ot_rewards: Dataset, \n",
    "             max_steps=1_000_000, seed=10, expectile=0.7, discount=0.99, temperature=3):\n",
    "\n",
    "    iql_agent_ot = Learner(\n",
    "            seed,\n",
    "            env_data.env.observation_space.sample()[np.newaxis],\n",
    "            env_data.env.action_space.sample()[np.newaxis],\n",
    "            max_steps=max_steps,\n",
    "            expectile=expectile,\n",
    "            discount=discount,\n",
    "            temperature=temperature)\n",
    "\n",
    "    pbar = tqdm(range(max_steps + 1))\n",
    "\n",
    "    for i in pbar:\n",
    "        sample = data_with_ot_rewards.sample(256)\n",
    "        batch = Batch(\n",
    "            observations=sample[\"observations\"],\n",
    "            next_observations=sample[\"next_observations\"],\n",
    "            actions = sample['actions'],\n",
    "            rewards= sample[\"rewards\"],\n",
    "            masks= sample[\"masks\"]\n",
    "        )\n",
    "        update_info = iql_agent_ot.update(batch)\n",
    "        update_info['adv'] = None\n",
    "\n",
    "        if i % 50_000 == 0 and i > 0:\n",
    "            eval_stats = evaluate(iql_agent_ot, env_data.env, num_episodes=10)[0]\n",
    "            eval_stats['return'] = env_data.env.get_normalized_score(eval_stats['return'])*100\n",
    "            print(eval_stats)\n",
    "            pbar.set_postfix(update_info)\n",
    "        if i % 2000 == 0:\n",
    "            pbar.set_postfix(update_info)\n",
    "\n",
    "    return iql_agent_ot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.utils import save_video\n",
    "\n",
    "def eval_agent(iql_agent_ot, env):\n",
    "\n",
    "    eval_stats = evaluate(iql_agent_ot, env, num_episodes=50)[0]\n",
    "    eval_stats['return'] = env.get_normalized_score(eval_stats['return'])*100\n",
    "    print(eval_stats)\n",
    "\n",
    "    frames=[]\n",
    "    i = 0\n",
    "    num_episodes = 1\n",
    "    all_reward = []\n",
    "    key = jax.random.PRNGKey(42)\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        episode_reward = 0\n",
    "        key, sample_key = jax.random.split(key, 2)\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        k = 0\n",
    "        while k < 2000:\n",
    "            key, sample_key = jax.random.split(sample_key, 2)\n",
    "            action = jax.device_get(iql_agent_ot.sample_actions(obs, temperature=0.0))\n",
    "            obs, reward, done ,_ = env.step(action)\n",
    "            os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "            frames.append(env.render(mode='rgb_array'))\n",
    "            os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "            episode_reward += reward\n",
    "            k+=1\n",
    "        all_reward.append(episode_reward)\n",
    "        print(episode_reward)\n",
    "    save_video.save_video(frames, video_folder='.', fps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def crossenv_expert_projection(\n",
    "    expert_obs, agent_obs, other_agent_obs, icvf_model_1, icvf_model_2 \n",
    "):\n",
    "    \n",
    "    Ne = expert_obs.shape[0]\n",
    "    cat_obs = jnp.concatenate([expert_obs, agent_obs], axis=0)\n",
    "    z_1 = eval_ensemble_psi(icvf_model_1, cat_obs).mean(axis=0)\n",
    "    z_2 = eval_ensemble_psi(icvf_model_2, other_agent_obs).mean(axis=0)\n",
    "\n",
    "    geom = pointcloud.PointCloud(z_1, z_2, epsilon=0.001)\n",
    "    ot_prob = linear_problem.LinearProblem(geom)\n",
    "    solver = sinkhorn.Sinkhorn(max_iterations=500, use_danskin=True)\n",
    "\n",
    "    ot_sink = solver(ot_prob)\n",
    "    P = ot_sink.matrix[:Ne, :] \n",
    "    # P = P ** 2\n",
    "    P = P / P.sum(1, keepdims=True)\n",
    "    \n",
    "    expert_proj = P @ z_2\n",
    "\n",
    "    return P, expert_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expert: halfcheetah, agent: hopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f262e6a785e4d7da00dfd05277a0605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ch_z = crossenv_expert_projection(\n",
    "    halfcheetah_data.expert_trajectory, \n",
    "    halfcheetah_data.agent_dataset.dataset_dict[\"observations\"][1000:10000],\n",
    "    hopper_data.agent_dataset.dataset_dict[\"observations\"][:10000],\n",
    "    halfcheetah_icvf_model.value_learner.model,\n",
    "    hopper_icvf_model.value_learner.model\n",
    ")\n",
    "\n",
    "expert = OTRewardsExpert(hopper_icvf_model, expert_z=ch_z)\n",
    "rewards = expert.compute_rewards(hopper_data.agent_dataset) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-178.75755 -13.217977 -0.011129665\n",
      "0.0 0.014802219 1.9343246\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1150d8fe11d4c899357a36b366b789d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'return': 38.82416872275892, 'length': 392.8}\n",
      "{'return': 49.755367148302575, 'length': 505.7}\n",
      "{'return': 46.706282574446696, 'length': 463.2}\n",
      "{'return': 70.35090313633712, 'length': 717.0}\n",
      "{'return': 60.658142989542185, 'length': 607.2}\n",
      "{'return': 81.90848792791941, 'length': 815.8}\n",
      "{'return': 71.75185532385508, 'length': 707.8}\n",
      "{'return': 75.36070078332473, 'length': 751.5}\n",
      "{'return': 65.40694374382271, 'length': 646.6}\n",
      "{'return': 59.86725287751574, 'length': 592.1}\n",
      "{'return': 61.54296942389733, 'length': 604.8}\n",
      "{'return': 69.85132003600141, 'length': 688.2}\n",
      "{'return': 59.981328877361726, 'length': 594.9}\n",
      "{'return': 64.85076510196555, 'length': 640.6}\n",
      "{'return': 63.28583581025741, 'length': 623.3}\n",
      "{'return': 57.05454035963865, 'length': 564.4}\n",
      "{'return': 60.55414480132802, 'length': 600.2}\n",
      "{'return': 61.95514110484371, 'length': 613.1}\n",
      "{'return': 70.81534563766094, 'length': 696.8}\n",
      "{'return': 61.910937713339834, 'length': 610.8}\n",
      "eval\n",
      "{'return': 63.25658803436569, 'length': 624.84}\n",
      "Found 5 GPUs for rendering. Using device 1.\n",
      "3514.1064703438838\n",
      "Moviepy - Building video /home/nazar/projects/AILOT/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/nazar/projects/AILOT/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/nazar/projects/AILOT/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "scaler = ExpRewardsScaler(a=2, b=3)\n",
    "train_data = make_train_data(hopper_data, rewards, scaler)\n",
    "iql_agent_ot = training(hopper_data, train_data)\n",
    "print(\"eval\")\n",
    "eval_agent(iql_agent_ot, hopper_data.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'return': 64.29731543022103, 'length': 634.28}\n",
      "3322.3044449708113\n",
      "Moviepy - Building video /home/nazar/projects/AILOT/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/nazar/projects/AILOT/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/nazar/projects/AILOT/rl-video-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "eval_agent(iql_agent_ot, hopper_data.env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expert: hopper, agent: halfcheetah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594b6c82109a40ac9bdabc4fadd212cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, hopper_z = crossenv_expert_projection(\n",
    "    hopper_data.expert_trajectory, \n",
    "    hopper_data.agent_dataset.dataset_dict[\"observations\"][1000:10000],\n",
    "    halfcheetah_data.agent_dataset.dataset_dict[\"observations\"][:10000],\n",
    "    hopper_icvf_model.value_learner.model,\n",
    "    halfcheetah_icvf_model.value_learner.model\n",
    ")\n",
    "\n",
    "expert = OTRewardsExpert(halfcheetah_icvf_model, expert_z=hopper_z)\n",
    "rewards = expert.compute_rewards(halfcheetah_data.agent_dataset) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-61.382805 -3.7067518 -0.11133348\n",
      "4.39356e-27 0.14845993 1.7892808\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e6f6cb1c654a929f5825052d87c9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'return': 37.34827159823951, 'length': 1000.0}\n",
      "{'return': 40.48865091324573, 'length': 1000.0}\n",
      "{'return': 41.7990364147947, 'length': 1000.0}\n",
      "{'return': 41.543602254839406, 'length': 1000.0}\n",
      "{'return': 42.651300590614674, 'length': 1000.0}\n",
      "{'return': 41.61175193042423, 'length': 1000.0}\n",
      "{'return': 43.4711161827794, 'length': 1000.0}\n",
      "{'return': 42.20855050684148, 'length': 1000.0}\n",
      "{'return': 39.29940227644949, 'length': 1000.0}\n",
      "{'return': 42.41720829914906, 'length': 1000.0}\n",
      "{'return': 43.93239964049964, 'length': 1000.0}\n",
      "{'return': 39.323755298910555, 'length': 1000.0}\n",
      "{'return': 42.69523902462256, 'length': 1000.0}\n",
      "{'return': 42.62258536438095, 'length': 1000.0}\n",
      "{'return': 42.952596216218026, 'length': 1000.0}\n",
      "{'return': 43.2527679022389, 'length': 1000.0}\n",
      "{'return': 43.63621280565453, 'length': 1000.0}\n",
      "{'return': 43.68034333090674, 'length': 1000.0}\n",
      "{'return': 43.047491575085346, 'length': 1000.0}\n",
      "{'return': 42.76063363612712, 'length': 1000.0}\n",
      "eval\n",
      "{'return': 42.91562946183661, 'length': 1000.0}\n",
      "10770.638864383462\n",
      "Moviepy - Building video /home/nazar/projects/AILOT/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/nazar/projects/AILOT/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/nazar/projects/AILOT/rl-video-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "scaler = ExpRewardsScaler(a=2, b=1)\n",
    "train_data = make_train_data(halfcheetah_data, rewards, scaler)\n",
    "iql_agent_ot = training(halfcheetah_data, train_data)\n",
    "print(\"eval\")\n",
    "eval_agent(iql_agent_ot, halfcheetah_data.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 21/21 [00:01<00:00, 17.75it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb719b718c64664a0d7fefe78b5ab4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/999000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert returns [5006.127595229074], mean 5006.127595229074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 21/21 [00:01<00:00, 17.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of terminal states: 1\n",
      "Number of terminal states: 1192\n",
      "Extra kwargs: {}\n"
     ]
    }
   ],
   "source": [
    "walker_data = load_env_data(\"walker2d-expert-v2\", \"walker2d-medium-v2\", expert_num=1)\n",
    "\n",
    "walker_icvf_model = icvf.create_eqx_learner(seed=228,\n",
    "                                     observations=walker_data.expert_dataset.dataset_dict['observations'][0],\n",
    "                                     hidden_dims=[256, 256],\n",
    "                                     pretrained_folder=\"walker2d-medium\",\n",
    "                                     load_pretrained_icvf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crossenv_expert_projection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, hopper_z \u001b[38;5;241m=\u001b[39m \u001b[43mcrossenv_expert_projection\u001b[49m(\n\u001b[1;32m      2\u001b[0m     hopper_data\u001b[38;5;241m.\u001b[39mexpert_trajectory, \n\u001b[1;32m      3\u001b[0m     hopper_data\u001b[38;5;241m.\u001b[39magent_dataset\u001b[38;5;241m.\u001b[39mdataset_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1000\u001b[39m:\u001b[38;5;241m10000\u001b[39m],\n\u001b[1;32m      4\u001b[0m     walker_data\u001b[38;5;241m.\u001b[39magent_dataset\u001b[38;5;241m.\u001b[39mdataset_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m10000\u001b[39m],\n\u001b[1;32m      5\u001b[0m     hopper_icvf_model\u001b[38;5;241m.\u001b[39mvalue_learner\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m      6\u001b[0m     walker_icvf_model\u001b[38;5;241m.\u001b[39mvalue_learner\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# w_z = walker_data.expert_intents(walker_icvf_model.value_learner.model)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m expert \u001b[38;5;241m=\u001b[39m OTRewardsExpert(walker_icvf_model, expert_z\u001b[38;5;241m=\u001b[39mhopper_z)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crossenv_expert_projection' is not defined"
     ]
    }
   ],
   "source": [
    "_, hopper_z = crossenv_expert_projection(\n",
    "    hopper_data.expert_trajectory, \n",
    "    hopper_data.agent_dataset.dataset_dict[\"observations\"][1000:10000],\n",
    "    walker_data.agent_dataset.dataset_dict[\"observations\"][:10000],\n",
    "    hopper_icvf_model.value_learner.model,\n",
    "    walker_icvf_model.value_learner.model\n",
    ")\n",
    "\n",
    "# w_z = walker_data.expert_intents(walker_icvf_model.value_learner.model)\n",
    "\n",
    "expert = OTRewardsExpert(walker_icvf_model, expert_z=hopper_z)\n",
    "rewards = expert.compute_rewards(walker_data.agent_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4977.6567 -6.78997 -0.047423262\n",
      "0.0 0.02343731 1.7347745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8dbf79eda3460bb95e372331439641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'return': 61.57563718817133, 'length': 754.8}\n",
      "{'return': 67.32047210715608, 'length': 820.3}\n",
      "{'return': 70.70063222285066, 'length': 860.6}\n",
      "{'return': 83.32679048477006, 'length': 1000.0}\n",
      "{'return': 82.1758230550337, 'length': 977.6}\n",
      "{'return': 76.11041875571046, 'length': 910.6}\n",
      "{'return': 83.15975493791889, 'length': 971.0}\n",
      "{'return': 78.42602295410624, 'length': 913.2}\n",
      "{'return': 82.86533033002696, 'length': 988.4}\n",
      "{'return': 79.04452918536423, 'length': 917.2}\n",
      "{'return': 84.36580700590643, 'length': 1000.0}\n",
      "{'return': 76.57034049739816, 'length': 880.6}\n",
      "{'return': 81.02505961178433, 'length': 948.6}\n",
      "{'return': 71.38968155521458, 'length': 845.0}\n",
      "{'return': 78.95277800020418, 'length': 910.1}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[188], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m scaler \u001b[38;5;241m=\u001b[39m ExpRewardsScaler(a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, b\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m make_train_data(walker_data, rewards, scaler)\n\u001b[0;32m----> 3\u001b[0m iql_agent_ot \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalker_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m eval_agent(iql_agent_ot, walker_data\u001b[38;5;241m.\u001b[39menv)\n",
      "Cell \u001b[0;32mIn[155], line 28\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(env_data, data_with_ot_rewards, max_steps, seed, expectile, discount, temperature)\u001b[0m\n\u001b[1;32m     20\u001b[0m sample \u001b[38;5;241m=\u001b[39m data_with_ot_rewards\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m     21\u001b[0m batch \u001b[38;5;241m=\u001b[39m Batch(\n\u001b[1;32m     22\u001b[0m     observations\u001b[38;5;241m=\u001b[39msample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     23\u001b[0m     next_observations\u001b[38;5;241m=\u001b[39msample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_observations\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     masks\u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m update_info \u001b[38;5;241m=\u001b[39m \u001b[43miql_agent_ot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m update_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50_000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/projects/AILOT/notebooks/../src/agents/iql_flax/learner.py:130\u001b[0m, in \u001b[0;36mLearner.update\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: Batch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m InfoDict:\n\u001b[0;32m--> 130\u001b[0m     new_rng, new_actor, new_critic, new_value, new_target_critic, info \u001b[38;5;241m=\u001b[39m \u001b[43m_update_jit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_critic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpectile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng \u001b[38;5;241m=\u001b[39m new_rng\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor \u001b[38;5;241m=\u001b[39m new_actor\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scaler = ExpRewardsScaler(a=2, b=3)\n",
    "train_data = make_train_data(walker_data, rewards, scaler)\n",
    "iql_agent_ot = training(walker_data, train_data)\n",
    "print(\"eval\")\n",
    "eval_agent(iql_agent_ot, walker_data.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icvf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
